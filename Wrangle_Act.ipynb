{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52491ddd",
   "metadata": {},
   "source": [
    "### WeRateDogs Data Wrangling\n",
    "\n",
    "#### Import Libraries & Authenticate API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import requests\n",
    "import configparser\n",
    "import json\n",
    "import time \n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14c06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2356"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load Twitter API credentials securely\n",
    "API_KEY = \"your_api_key_here\"\n",
    "API_SECRET = \"your_api_secret_here\"\n",
    "ACCESS_TOKEN = \"your_access_token_here\"\n",
    "ACCESS_SECRET = \"your_access_secret_here\"\n",
    "\n",
    "# Authenticate with Twitter\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "tweet_ids = archive.tweet_id.values\n",
    "len(tweet_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ddccd2",
   "metadata": {},
   "source": [
    "#### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>39467</td>\n",
       "      <td>8853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>892177421306343426</td>\n",
       "      <td>33819</td>\n",
       "      <td>6514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>891815181378084864</td>\n",
       "      <td>25461</td>\n",
       "      <td>4328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>891689557279858688</td>\n",
       "      <td>42908</td>\n",
       "      <td>8964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>891327558926688256</td>\n",
       "      <td>41048</td>\n",
       "      <td>9774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  likes  retweets\n",
       "0  892420643555336193  39467      8853\n",
       "1  892177421306343426  33819      6514\n",
       "2  891815181378084864  25461      4328\n",
       "3  891689557279858688  42908      8964\n",
       "4  891327558926688256  41048      9774"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to download and read the image predictions dataset\n",
    "def download_image_predictions(url, filename):\n",
    "    \"\"\"Downloads the image predictions dataset and saves it as a file.\"\"\"\n",
    "    response = requests.get(url, allow_redirects=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return pd.read_csv(filename, sep='\\t')\n",
    "    else:\n",
    "        print(\"Error downloading file.\")\n",
    "        return None\n",
    "\n",
    "# Load Twitter archive dataset\n",
    "archive = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "\n",
    "# Download image predictions dataset\n",
    "image_predictions_url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "image_predictions = download_image_predictions(image_predictions_url, 'image-predictions.tsv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9f1524",
   "metadata": {},
   "source": [
    "#### Fetch Twitter API Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00aa93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "tweet_ids = archive[\"tweet_id\"].values\n",
    "output_file = \"tweet_json.txt\"\n",
    "fails_dict = {}\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Fetching {len(tweet_ids)} tweets from Twitter API...\\n\")\n",
    "\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    for count, tweet_id in enumerate(tweet_ids, start=1):\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode=\"extended\")\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write(\"\\n\")  # Ensure each tweet is stored in a new line\n",
    "            print(f\"✔ {count}: Successfully fetched tweet ID {tweet_id}\")\n",
    "        except tweepy.TweepError as e:\n",
    "            print(f\"✖ {count}: Failed to fetch tweet ID {tweet_id} - {e}\")\n",
    "            fails_dict[tweet_id] = str(e)  # Store the error message for debugging\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\n--------------------------------------------\")\n",
    "print(f\"Completed in {round(end_time - start_time, 2)} seconds.\")\n",
    "print(f\"Failed tweets: {len(fails_dict)}\")\n",
    "print(\"--------------------------------------------\\n\")\n",
    "\n",
    "# Save failed tweet IDs for reprocessing\n",
    "with open(\"failed_tweets.json\", \"w\") as failed_file:\n",
    "    json.dump(fails_dict, failed_file, indent=4)\n",
    "\n",
    "# Quick data overview\n",
    "def summarize_data(df, name):\n",
    "    \"\"\"Prints a quick summary of a dataframe.\"\"\"\n",
    "    print(f\"\\nSummary for {name}:\")\n",
    "    print(df.info())\n",
    "    print(df.head())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "summarize_data(archive, \"Twitter Archive\")\n",
    "\n",
    "summarize_data(image_predictions, \"Image Predictions\")\n",
    "\n",
    "# Data Cleaning - Removing Retweets\n",
    "archive_cleaned = archive[archive.retweeted_status_id.isna()].copy()\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id',\n",
    "    'retweeted_status_user_id', 'retweeted_status_timestamp'\n",
    "]\n",
    "archive_cleaned.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Converting timestamp to datetime format\n",
    "archive_cleaned['timestamp'] = pd.to_datetime(archive_cleaned['timestamp'])\n",
    "\n",
    "# Fixing rating inconsistencies\n",
    "def extract_ratings(text):\n",
    "    \"\"\"Extracts rating numerator and denominator from tweet text.\"\"\"\n",
    "    match = re.search(r\"(\\d+)/(\\d+)\", text)\n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2))\n",
    "    return None, None\n",
    "\n",
    "archive_cleaned[['rating_numerator', 'rating_denominator']] = archive_cleaned[\n",
    "    'text'\n",
    "].apply(lambda x: pd.Series(extract_ratings(x)))\n",
    "\n",
    "# Filtering out invalid ratings\n",
    "archive_cleaned = archive_cleaned[archive_cleaned['rating_denominator'] == 10]\n",
    "\n",
    "# Merging datasets\n",
    "final_df = archive_cleaned.merge(image_predictions, on='tweet_id', how='left')\n",
    "\n",
    "# Saving cleaned dataset\n",
    "final_df.to_csv(\"twitter_archive_master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e419df",
   "metadata": {},
   "source": [
    "#### Data Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61731df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data overview\n",
    "def summarize_data(df, name):\n",
    "    \"\"\"Prints a quick summary of a dataframe.\"\"\"\n",
    "    print(f\"Summary for {name}:\")\n",
    "    print(df.info())\n",
    "    print(df.head())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "summarize_data(archive, \"Twitter Archive\")\n",
    "summarize_data(image_predictions, \"Image Predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde539cd",
   "metadata": {},
   "source": [
    "#### Programatic Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b49f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2356 entries, 0 to 2355\n",
      "Data columns (total 17 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   tweet_id                    2356 non-null   int64  \n",
      " 1   in_reply_to_status_id       78 non-null     float64\n",
      " 2   in_reply_to_user_id         78 non-null     float64\n",
      " 3   timestamp                   2356 non-null   object \n",
      " 4   source                      2356 non-null   object \n",
      " 5   text                        2356 non-null   object \n",
      " 6   retweeted_status_id         181 non-null    float64\n",
      " 7   retweeted_status_user_id    181 non-null    float64\n",
      " 8   retweeted_status_timestamp  181 non-null    object \n",
      " 9   expanded_urls               2297 non-null   object \n",
      " 10  rating_numerator            2356 non-null   int64  \n",
      " 11  rating_denominator          2356 non-null   int64  \n",
      " 12  name                        2356 non-null   object \n",
      " 13  doggo                       2356 non-null   object \n",
      " 14  floofer                     2356 non-null   object \n",
      " 15  pupper                      2356 non-null   object \n",
      " 16  puppo                       2356 non-null   object \n",
      "dtypes: float64(4), int64(3), object(10)\n",
      "memory usage: 313.0+ KB\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sys\n",
    "\n",
    "# Capture and format output\n",
    "buffer = io.StringIO()\n",
    "sys.stdout = buffer\n",
    "archive.info()\n",
    "sys.stdout = sys.__stdout__\n",
    "\n",
    "# Print formatted output\n",
    "print(\"\\n**Programmatic Assessment of `archive` Dataset:**\")\n",
    "print(buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3114dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 Rows of Archive Data\n",
    "display(archive.head())\n",
    "\n",
    "# Checking Reply Status ID, User ID, and Source\n",
    "archive_subset = archive[['in_reply_to_status_id', 'in_reply_to_user_id', 'source']]\n",
    "display(archive_subset.tail())\n",
    "\n",
    "# Expanded URLs\n",
    "display(archive['expanded_urls'].head())\n",
    "\n",
    "# Rating Numerator Counts\n",
    "display(archive.rating_numerator.value_counts())\n",
    "\n",
    "# Rating Denominator Counts\n",
    "display(archive.rating_denominator.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66def887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Potentially Invalid Dog Names\n",
    "display(archive[archive.name.str.islower() == True].name.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Predictions Data Overview\n",
    "print(image_predictions.info())\n",
    "display(image_predictions.head())\n",
    "display(image_predictions.describe())\n",
    "\n",
    "# Checking Prediction Classifications\n",
    "prediction_img_df = image_predictions[['p1', 'p2', 'p3']]\n",
    "display(prediction_img_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2330352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Dog Predictions\n",
    "num_p1_notdog = image_predictions.p1_dog.value_counts().get(False, 0)\n",
    "num_p2_notdog = image_predictions.p2_dog.value_counts().get(False, 0)\n",
    "num_p3_notdog = image_predictions.p3_dog.value_counts().get(False, 0)\n",
    "print(f\" Non-Dog Predictions - P1: {num_p1_notdog}, P2: {num_p2_notdog}, P3: {num_p3_notdog}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API JSON Data Overview\n",
    "jsonlist = pd.read_json(\"tweet_json.txt\", lines=True)\n",
    "print(jsonlist.info())\n",
    "display(jsonlist.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed9143",
   "metadata": {},
   "source": [
    "## Data Quality Issues\n",
    "During the assessment of the dataset, the following quality issues were identified:\n",
    "\n",
    "### Twitter Archive\n",
    "1. **Timestamp Formatting**  \n",
    "   - The `timestamp` and `retweeted_status_timestamp` columns are stored as objects (string format) instead of `datetime`, requiring conversion.\n",
    "\n",
    "2. **Rating Outliers**  \n",
    "   - Some ratings have denominators greater than 10. While higher-than-10 ratings are a humorous feature of WeRateDogs, some extreme outliers exist and require review.\n",
    "\n",
    "3. **Invalid Dog Names**  \n",
    "   - Several names are incorrect or placeholders (e.g., `\"a\"`, `\"the\"`, `\"an\"`), likely due to parsing errors.\n",
    "\n",
    "4. **Columns with Mostly Null Values**  \n",
    "   - The following columns contain mostly missing data and may not be useful:  \n",
    "     - `in_reply_to_status_id` (78 non-null)  \n",
    "     - `in_reply_to_user_id` (78 non-null)  \n",
    "     - `retweeted_status_id` (181 non-null)  \n",
    "     - `retweeted_status_user_id` (181 non-null)  \n",
    "     - `retweeted_status_timestamp` (181 non-null)  \n",
    "\n",
    "5. **Source Column Contains HTML**  \n",
    "   - The `source` column includes raw HTML tags, which should be extracted for readability.\n",
    "\n",
    "6. **Missing URLs**  \n",
    "   - There are only 2,297 URLs, but the dataset contains 2,356 total entries, suggesting that some URLs are missing.\n",
    "\n",
    "### Image Predictions File\n",
    "7. **Missing Predictions**  \n",
    "   - The Twitter archive has 2,356 tweets, but the image predictions dataset contains only 2,075 entries, meaning some tweets lack image-based classification.\n",
    "\n",
    "8. **Tweet ID Data Type**  \n",
    "   - The `tweet_id` column should be stored as a string, not an integer, to prevent unintended conversions and maintain consistency.\n",
    "\n",
    "### API Data File\n",
    "9. **Missing API Data Entries**  \n",
    "   - The API data contains 2,354 entries, meaning two tweets are missing.\n",
    "\n",
    "10. **Tweet ID Data Type**  \n",
    "   - Similar to the image predictions file, the `tweet_id` column should be stored as a string instead of an integer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9cea9",
   "metadata": {},
   "source": [
    "## Tidiness Issues\n",
    "1. **Dog Stages Should Be Combined**  \n",
    "   - The `doggo`, `floofer`, `pupper`, and `puppo` columns should be consolidated into a single column representing dog stages.\n",
    "\n",
    "2. **Files Should Be Merged**  \n",
    "   - The Twitter archive, image predictions, and API data files should be combined into a single, unified dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84af802",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "In this section, all documented **quality and tidiness issues** will be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35721c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the original datasets to preserve raw data\n",
    "df_archive = archive.copy()\n",
    "df_predict = image_predictions.copy()\n",
    "df_api = jsonlist.copy()\n",
    "\n",
    "# Display first row of the Twitter archive dataset\n",
    "df_archive.head(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce502511",
   "metadata": {},
   "source": [
    "### Changing datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f589a",
   "metadata": {},
   "source": [
    "##### Define: Some columns have datatypes that are incorrect. \n",
    "Convert `tweet_id` to **string** in all datasets.\n",
    "Convert `timestamp` to **datetime**.\n",
    "Convert `rating_numerator` to **float**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0846d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweet_id to string\n",
    "df_archive['tweet_id'] = df_archive['tweet_id'].astype(str)\n",
    "df_predict['tweet_id'] = df_predict['tweet_id'].astype(str)\n",
    "df_api['tweet_id'] = df_api['tweet_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "df_archive['timestamp'] = pd.to_datetime(df_archive['timestamp'])\n",
    "\n",
    "# Convert rating_numerator to float\n",
    "df_archive['rating_numerator'] = df_archive['rating_numerator'].asty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data type changes\n",
    "df_archive.info()\n",
    "df_predict.info()\n",
    "df_api.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e64ccb",
   "metadata": {},
   "source": [
    "### **Handling Rating Denominator Inconsistencies**\n",
    "\n",
    ">The WeRateDogs scoring system is a N/10 system with N being a number that is often greater than 10 due to the nature of the humorous scoring system. \n",
    "\n",
    ">Some denominator values exceet 10, which is not expected.\n",
    "\n",
    ">Normalize ratings by adjusting values to fit the standard scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32aaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify tweets where denominator is not 10\n",
    "discrepant_ratings = df_archive[df_archive['rating_denominator'] != 10]\n",
    "discrepant_ratings\n",
    "\n",
    "# Normalize ratings to a /10 scale\n",
    "df_archive['rating_numerator'] = (df_archive['rating_numerator'] / df_archive['rating_denominator']) * 10\n",
    "df_archive['rating_denominator'] = 10\n",
    "\n",
    "# Verify changes\n",
    "df_archive[['tweet_id', 'rating_numerator', 'rating_denominator']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5071ee",
   "metadata": {},
   "source": [
    "### **Merging Datasets and Storing Data**\n",
    "\n",
    "> Merge `df_archive`, `df_predict`, and `df_api` on `tweet_id`.\n",
    "\n",
    "> Use **left join** to preserve all tweet archive data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3726ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on tweet_id\n",
    "final_df = df_archive.merge(df_predict, on='tweet_id', how='left')\n",
    "final_df = final_df.merge(df_api, on='tweet_id', how='left')\n",
    "\n",
    "# Save cleaned dataset\n",
    "final_df.to_csv(\"twitter_archive_master.csv\", index=False)\n",
    "\n",
    "# Verify merged dataset structure\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a798cfb",
   "metadata": {},
   "source": [
    "### **Cleaning HTML from Source Column**\n",
    "\n",
    "> The `source` column contains **HTML anchor tags** that should be extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd912f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source(html_string):\n",
    "    \"\"\"Extracts text from an HTML anchor tag.\"\"\"\n",
    "    match = re.search(r\">(.*?)<\", html_string)\n",
    "    return match.group(1) if match else html_string\n",
    "\n",
    "# Apply function to source column\n",
    "df_archive['source'] = df_archive['source'].apply(extract_source)\n",
    "\n",
    "# Verify changes\n",
    "df_archive['source'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad4904",
   "metadata": {},
   "source": [
    "### **Handling Missing Data**\n",
    "\n",
    "> Drop columns with **mostly missing values**.\n",
    "\n",
    "> Ensure URLs are properly filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop columns with excessive missing values\n",
    "columns_to_drop = [\n",
    "    'in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id',\n",
    "    'retweeted_status_user_id', 'retweeted_status_timestamp'\n",
    "]\n",
    "df_archive.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Fill missing URLs with 'No URL Available'\n",
    "df_archive['expanded_urls'].fillna(\"No URL Available\", inplace=True)\n",
    "\n",
    "# Verify missing data is handled\n",
    "df_archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4af76d",
   "metadata": {},
   "source": [
    "### **Final Dataset Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display final cleaned dataset structure and first few rows\n",
    "final_df.info()\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bbde3",
   "metadata": {},
   "source": [
    "### **Summary of Cleaning Steps**\n",
    "\n",
    "-Converted data types where needed.\n",
    "\n",
    "-Handled rating inconsistencies by normalizing to `/10` scale.\n",
    "\n",
    "-**Merged datasets** to create a final master DataFrame.\n",
    "\n",
    "-Extracted clean text from HTML in the `source` column.\n",
    "\n",
    "-Removed highly null columns and ensured URLs are complete.\n",
    "\n",
    "-Saved the cleaned dataset for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065974b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Analyzing and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics of the Cleaned Dataset\n",
    "final_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a562e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Dog Names\n",
    "final_df['name'].value_counts().head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4087d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Tweet Sources\n",
    "final_df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bbc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Percentage of Non-Dog Predictions for p1, p2, and p3\n",
    "notdog1 = final_df.p1_dog.value_counts(normalize=True).mul(100)[False]\n",
    "notdog2 = final_df.p2_dog.value_counts(normalize=True).mul(100)[False]\n",
    "notdog3 = final_df.p3_dog.value_counts(normalize=True).mul(100)[False]\n",
    "\n",
    "# Store results in a list\n",
    "notdogvals = [notdog1, notdog2, notdog3]\n",
    "notdogvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Retweeted Dog Names\n",
    "final_df.groupby('name')['retweets'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Most Retweeted Dog Labels\n",
    "final_df.groupby('label')['retweets'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee571ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Non-Name Values for Cleaner Visualizations\n",
    "final_df['name'] = final_df['name'].apply(lambda x: 'Unknown' if x.lower() in ['a', 'the', 'an', 'by'] else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a1f9c",
   "metadata": {},
   "source": [
    "### **Summary of Findings**\n",
    "- **Charlie** is the most common dog name.\n",
    "- **iPhone is the most used platform** for tweeting.\n",
    "- **A high percentage of third predictions are non-dogs.**\n",
    "- **Bo, Stephan, and Duddles are the most retweeted dog names.**\n",
    "- **Pupper is the most abundant label.**\n",
    "\n",
    "Further visualizations will be conducted to support these findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa815cf",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "The most interesting insights involve the number of retweets for different dog names and labels. The following visualizations highlight these trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba4a3d",
   "metadata": {},
   "source": [
    "#### Retweeted Dog Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Aggregate total retweets by dog name, excluding 'None' and 'No Name'\n",
    "name_retweets = final_df.groupby('name')['retweets'].sum().sort_values(ascending=False).nlargest(7)\n",
    "name_retweets = name_retweets.drop(index=['None', 'No Name'], errors='ignore')\n",
    "\n",
    "# Plot the most retweeted dog names\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=name_retweets.index, y=name_retweets.values, palette='viridis')\n",
    "plt.title(\"Most Retweeted Dog Names\")\n",
    "plt.xlabel(\"Dog Names\")\n",
    "plt.ylabel(\"Number of Retweets\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(\"most_retweeted_dog_names.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a209efb",
   "metadata": {},
   "source": [
    "##### Retweeted Dog Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabfbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate total retweets by dog label\n",
    "dog_label = final_df.groupby('label')['retweets'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Plot the most retweeted dog labels\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=dog_label.index[:10], y=dog_label.values[:10], palette='magma')\n",
    "plt.title(\"Most Retweeted Dog Labels\")\n",
    "plt.xlabel(\"Dog Labels\")\n",
    "plt.ylabel(\"Number of Retweets\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(\"most_retweeted_dog_labels.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c14c32",
   "metadata": {},
   "source": [
    "These visualizations provide a clearer understanding of the engagement levels associated with different dog names and classifications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
